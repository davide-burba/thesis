{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepHit\n",
    "\n",
    "This notebook implements the DeepHit survival model for Heart Failure patients (with functional covariates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from lifelines.utils import concordance_index\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.deep_hit_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('../../data/main_process_preprocessed_data.csv')\n",
    "test = pd.read_csv('../../data/main_process_preprocessed_data_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Arbitrary) maximum survival time: 10.0 years\n",
      "Number of time steps: 60\n",
      "Size time step: 2.0 months\n"
     ]
    }
   ],
   "source": [
    "# set time windows parameters\n",
    "T_max = 365*10\n",
    "n_times = int(T_max/60)\n",
    "# discretise time\n",
    "df['discretised_time_event'] = [int(v) for v in df.time_event/T_max*n_times]\n",
    "\n",
    "print('(Arbitrary) maximum survival time:',np.round(T_max/365,1),'years')\n",
    "print('Number of time steps:',n_times)\n",
    "print('Size time step:',np.round(T_max/n_times/30,1),'months')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy sex\n",
    "df['sexM'] = [1 if v == 'M' else 0 for v in df.sex]\n",
    "test['sexM'] = [1 if v == 'M' else 0 for v in test.sex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set features\n",
    "features = ['sexM', 'age_in','ACE_PC1', 'ACE_PC2','beta_PC1', 'beta_PC2','aldosteronics_PC1','aldosteronics_PC2','hospitalisation_PC1', 'hospitalisation_PC2']\n",
    "\n",
    "X,X_test = df[features],test[features]\n",
    "y = df.discretised_time_event\n",
    "true_times = df.time_event\n",
    "status = df.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale\n",
    "mean = X.mean()\n",
    "std = X.std()\n",
    "X -= mean\n",
    "X /= std\n",
    "X_test -= mean\n",
    "X_test /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train in train/validation (for early stopping)\n",
    "X_train, X_valid, y_train, y_valid, status_train, status_valid = \\\n",
    "    train_test_split(X, y, status, test_size=0.15, random_state=47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform validation sets in tensors\n",
    "X_valid = torch.from_numpy(X_valid.values.astype('float32'))\n",
    "y_valid = torch.from_numpy(y_valid.values)\n",
    "status_valid = torch.from_numpy(status_valid.values.astype('float32'))\n",
    "X_valid,y_valid,status_valid = X_valid.to(device),y_valid.to(device),status_valid.to(device)\n",
    "\n",
    "# transform test set in tensors\n",
    "X_test = torch.from_numpy(X_test.values.astype('float32'))\n",
    "X_test = X_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sexM</th>\n",
       "      <th>age_in</th>\n",
       "      <th>ACE_PC1</th>\n",
       "      <th>ACE_PC2</th>\n",
       "      <th>beta_PC1</th>\n",
       "      <th>beta_PC2</th>\n",
       "      <th>aldosteronics_PC1</th>\n",
       "      <th>aldosteronics_PC2</th>\n",
       "      <th>hospitalisation_PC1</th>\n",
       "      <th>hospitalisation_PC2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>-1.067212</td>\n",
       "      <td>0.704192</td>\n",
       "      <td>-0.076368</td>\n",
       "      <td>-2.165450</td>\n",
       "      <td>-0.248604</td>\n",
       "      <td>0.147316</td>\n",
       "      <td>-0.673443</td>\n",
       "      <td>0.197863</td>\n",
       "      <td>0.059408</td>\n",
       "      <td>0.607386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>0.936726</td>\n",
       "      <td>-2.141105</td>\n",
       "      <td>-0.953714</td>\n",
       "      <td>0.265713</td>\n",
       "      <td>0.327546</td>\n",
       "      <td>2.283676</td>\n",
       "      <td>1.553493</td>\n",
       "      <td>0.570840</td>\n",
       "      <td>0.673866</td>\n",
       "      <td>1.216279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>-1.067212</td>\n",
       "      <td>-1.340865</td>\n",
       "      <td>-0.966636</td>\n",
       "      <td>0.239129</td>\n",
       "      <td>1.447139</td>\n",
       "      <td>-2.246391</td>\n",
       "      <td>-0.673443</td>\n",
       "      <td>0.197863</td>\n",
       "      <td>1.433501</td>\n",
       "      <td>1.821656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>-1.067212</td>\n",
       "      <td>-0.362794</td>\n",
       "      <td>1.186611</td>\n",
       "      <td>-2.617261</td>\n",
       "      <td>0.136343</td>\n",
       "      <td>-0.435556</td>\n",
       "      <td>-0.673443</td>\n",
       "      <td>0.197863</td>\n",
       "      <td>0.959095</td>\n",
       "      <td>0.953818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>-1.067212</td>\n",
       "      <td>0.882023</td>\n",
       "      <td>0.397882</td>\n",
       "      <td>1.557940</td>\n",
       "      <td>0.332961</td>\n",
       "      <td>1.385526</td>\n",
       "      <td>-0.673443</td>\n",
       "      <td>0.197863</td>\n",
       "      <td>-0.363884</td>\n",
       "      <td>-0.577222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          sexM    age_in   ACE_PC1   ACE_PC2  beta_PC1  beta_PC2  \\\n",
       "636  -1.067212  0.704192 -0.076368 -2.165450 -0.248604  0.147316   \n",
       "1602  0.936726 -2.141105 -0.953714  0.265713  0.327546  2.283676   \n",
       "280  -1.067212 -1.340865 -0.966636  0.239129  1.447139 -2.246391   \n",
       "1800 -1.067212 -0.362794  1.186611 -2.617261  0.136343 -0.435556   \n",
       "412  -1.067212  0.882023  0.397882  1.557940  0.332961  1.385526   \n",
       "\n",
       "      aldosteronics_PC1  aldosteronics_PC2  hospitalisation_PC1  \\\n",
       "636           -0.673443           0.197863             0.059408   \n",
       "1602           1.553493           0.570840             0.673866   \n",
       "280           -0.673443           0.197863             1.433501   \n",
       "1800          -0.673443           0.197863             0.959095   \n",
       "412           -0.673443           0.197863            -0.363884   \n",
       "\n",
       "      hospitalisation_PC2  \n",
       "636              0.607386  \n",
       "1602             1.216279  \n",
       "280              1.821656  \n",
       "1800             0.953818  \n",
       "412             -0.577222  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se early stopping parameter\n",
    "max_epochs_no_improvement = 150\n",
    "\n",
    "# build a data loader\n",
    "trainds= ColumnarDataset(X_train, y_train,status_train) \n",
    "params = {'batch_size': 50,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 4}\n",
    " \n",
    "train_dl = DataLoader(trainds, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyperparameters sigma and alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = [1e2,1e3,1e4,1e5]\n",
    "alphas = [0.5,1,1.5,2,2.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [1:40:21<9:06:00, 1927.11s/it]"
     ]
    }
   ],
   "source": [
    "hyp_scores = []\n",
    "\n",
    "# prepare list of hyper-parameters tuples\n",
    "hyper_params = []\n",
    "for sigma in sigmas:\n",
    "    for alpha in alphas:\n",
    "        hyper_params.append((sigma,alpha))\n",
    "        \n",
    "# loop over hyper-parameters tuples\n",
    "for sigma,alpha in tqdm(hyper_params):\n",
    "        \n",
    "        # set loss hyperparams:\n",
    "        criterion = Surv_Loss(sigma,alpha)\n",
    "        \n",
    "        # build the network\n",
    "        net = DeepHit(len(features),n_times).to(device)\n",
    "        # initialize weights\n",
    "        nn.init.xavier_normal_(net.fc1.weight)  \n",
    "        nn.init.xavier_normal_(net.fc2.weight)  \n",
    "        nn.init.xavier_normal_(net.fc3.weight)  \n",
    "        # set optimizer\n",
    "        optimizer = optim.Adam(net.parameters(), lr=1e-4)\n",
    "        \n",
    "        \n",
    "        #train        \n",
    "        train_losses,valid_losses = [],[]\n",
    "        best_net = DeepHit(len(features),n_times)\n",
    "        best_loss = 1e10\n",
    "        epochs_no_improvement = 0\n",
    "        for epoch in range(5000):  # loop over the dataset multiple times\n",
    "            running_loss = 0.0\n",
    "\n",
    "            net.train()\n",
    "            for i, data in enumerate(train_dl, 0):\n",
    "                inputs, labels, status = data\n",
    "                # make it use GPU, if you have it\n",
    "                inputs, labels, status = inputs.to(device), labels.to(device), status.to(device) \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward + backward + optimize\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels,status)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # training loss\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            # Evaluate on validation set\n",
    "            net.eval()\n",
    "            valid_pred = net(X_valid)\n",
    "            valid_loss = criterion(valid_pred, y_valid,status_valid).item()\n",
    "\n",
    "            # store loss\n",
    "            train_losses.append(running_loss)\n",
    "            valid_losses.append(valid_loss)\n",
    "\n",
    "            # early stopping\n",
    "            if valid_loss < best_loss:\n",
    "                best_loss = valid_loss\n",
    "                # save weights\n",
    "                best_net.load_state_dict(net.state_dict())\n",
    "                epochs_no_improvement = 0\n",
    "            else:\n",
    "                epochs_no_improvement += 1\n",
    "\n",
    "            if epochs_no_improvement > max_epochs_no_improvement:\n",
    "                break\n",
    "\n",
    "        best_net.eval()\n",
    "        valid_pred = best_net(X_valid)\n",
    "        expected_survival_times_valid = compute_expected_survival_time(valid_pred,n_times,T_max)\n",
    "        C_valid = concordance_index(y_valid,expected_survival_times_valid,status_valid)\n",
    "        hyp_scores.append((sigma,alpha,C_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_C_overall = max([v[2] for v in hyp_scores])\n",
    "for v in hyp_scores:\n",
    "    if v[2] == best_C_overall:\n",
    "        print('sigma:',v[0],' alpha:',v[1],'  C-index:', v[2],'    <---- best hyper-parameters choice')\n",
    "        best_sigma = v[0]\n",
    "        best_alpha = v[1]\n",
    "    else:\n",
    "        print('sigma:',v[0],' alpha:',v[1],'  C-index:', v[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set criterion: best hyper-params choice\n",
    "criterion = Surv_Loss(best_sigma,best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the network\n",
    "net = DeepHit(len(features),n_times).to(device)\n",
    "# initialize weights\n",
    "nn.init.xavier_normal_(net.fc1.weight)  \n",
    "nn.init.xavier_normal_(net.fc2.weight)  \n",
    "nn.init.xavier_normal_(net.fc3.weight)  \n",
    "# set optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses,valid_losses = [],[]\n",
    "best_net = DeepHit(len(features),n_times)\n",
    "best_loss = 1e10\n",
    "epochs_no_improvement = 0\n",
    "\n",
    "#train\n",
    "for epoch in range(5000):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    net.train()\n",
    "    for i, data in enumerate(train_dl, 0):\n",
    "        inputs, labels, status = data\n",
    "        # make it use GPU, if you have it\n",
    "        inputs, labels, status = inputs.to(device), labels.to(device), status.to(device) \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels,status)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # training loss\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    net.eval()\n",
    "    valid_pred = net(X_valid)\n",
    "    valid_loss = criterion(valid_pred, y_valid,status_valid).item()\n",
    "    \n",
    "    # store loss\n",
    "    train_losses.append(running_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    # early stopping\n",
    "    if valid_loss < best_loss:\n",
    "        best_loss = valid_loss\n",
    "        # save weights\n",
    "        best_net.load_state_dict(net.state_dict())\n",
    "        epochs_no_improvement = 0\n",
    "    else:\n",
    "        epochs_no_improvement += 1\n",
    "        \n",
    "    if epochs_no_improvement > max_epochs_no_improvement:\n",
    "        break\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch: ',epoch,'   Training Loss: ',np.round(running_loss,2),'   Current Validation Loss: ',np.round(valid_loss,2),'   Best Validation Loss: ',np.round(best_loss,2))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, color = 'orange', label = 'training')\n",
    "plt.legend()\n",
    "plt.title('Survival loss')\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(valid_losses, color = 'blue', label = 'validation')\n",
    "plt.legend()\n",
    "plt.title('Survival loss')\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Concordance Index on test set\n",
    "\n",
    "Note: we convert predictions to original time scale and we compare with true time of events; this is done in order to make it comparable with other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_net.eval()\n",
    "prediction_test_set = best_net(X_test)\n",
    "expected_survival_times = compute_expected_survival_time(prediction_test_set,n_times,T_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = concordance_index(test.time_event, \n",
    "                  expected_survival_times, \n",
    "                  test.status)\n",
    "\n",
    "print('Concordance Index on test set:',np.round(C*100,2),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(best_net.state_dict(), '../../data/deepHit_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
